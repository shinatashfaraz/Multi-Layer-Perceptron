# -*- coding: utf-8 -*-
"""gradient-descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HPUNrX5tQJ2Mv9M0fp_2rvfqJb0r1oQI

### Generate Dataset
"""

from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D

"""write the function bellow for creating num_sample sample between range_start and range_end.

y come from a line with slop=4 and intercept = 5
for creating Y use Gussion noise with std = noise_std

use linspace for creating X

"""

np.random.seed(42)

def generate_dataset(range_start, range_end, num_sample, noise_std = 5) :
  x = np.linspace(range_start, range_end, num_sample)
  noise = np.random.normal(0, noise_std, num_sample)
  y = (4*x) + 5 + noise
  return x,y

"""Create 50 sample between -10 and 10 as X
Use Gussian noise with std=5 for creating y
"""

X,y = generate_dataset(-10,10,50)

"""visualize the data in an scatter plot"""

x_line = np.linspace(min(X), max(X), 100)
y_line = 4 * x_line + 5

plt.scatter(X, y)
plt.plot(x_line, y_line, color='green', label='y = 4x + 5')  # Add the line
plt.xlabel('X')
plt.ylabel('y')
plt.title('Generated Dataset')
plt.grid(True)
plt.legend()

plt.show()

"""write the function bellow to compute the h_w or y_hat,
implement it with inner product. suppose dimensions of X are [numFeatures, numSamples]
"""

def h_w(X,w) :
  return (w[1]*X) + w[0]

"""cost function : get X, y , w as input and return the J(w) = MSE"""

def cost_function(X,y,w):
  hw = h_w(X,w)
  return np.mean((hw-y)**2)

"""### Visualize cost function

plot 3D surfaces:
the function to plot the 3D surfaces is plot_surface(X,Y,Z), where X and Y are the output arrays from meshgrid, and Z=f(X,Y) or Z(i,j)=f(X(i,j),Y(i,j))

for plotting in 3d the axes must projected to 3d with the instruction bellow :
ax = plt.axes(projection='3d')
and then use the plot_surface function of the ax to plot 3d surface
"""

w0_vals = np.linspace(-10, 10, 200)
w1_vals = np.linspace(-1, 6, 200)
J = np.zeros((len(w0_vals),len(w1_vals)))
for i in range(len(w0_vals)):
  for j in range(len(w1_vals)):
    w = np.array([w0_vals[i],w1_vals[j]])
    J[i,j] = cost_function(X,y,w)

"""visualize cost function using log scale"""

ax = plt.axes(projection = '3d')
w0,w1 = np.meshgrid(w0_vals , w1_vals)
ax.plot_surface(w0,w1,J)

ax = plt.axes(projection='3d')
w0,w1 = np.meshgrid(w0_vals,w1_vals)
plt.title('Cost Function')
plt.xlabel('w0')
plt.ylabel('w1')
ax.plot_surface(w0,w1,np.log(J), cmap='viridis')
ax.contourf(w0,w1,np.log(J),zdir='z',offset=np.log(J).min(),cmap='viridis')
plt.show()

"""### Gradient Descent:

gradient_descent function get X,y,w,alpha,num_iters as input and return final w, cost_history and w_history as output
"""

def gradient_descent(X, y, w, alpha, num_iters):
  #note to use the instruction bellow to store the Ws
  #w_history.append(w.copy())
  cost_history = []
  w_history = []
  for i in range(num_iters):
    w_history.append(w.copy())
    hw = h_w(X,w)
    w[0] = w[0] - alpha * (2/X.shape[0]) * np.sum(hw - y)
    w[1] = w[1] - alpha * (2/X.shape[0]) * np.sum(X*(hw - y))
    cost_history.append(cost_function(X,y,w))

  return w, cost_history, w_history

"""initialize w to [0,0] and run gradient_descent for alpha = 0.05 and 500 iteration"""

alpha = 0.0001
num_iters = 10000
w, cost_history, w_history = gradient_descent(X, y, [0,0], alpha, num_iters)
print(w)

"""### Plot GD Progression (without labels for lines, different alphas(alpha here is plot function argument))"""

x_line = np.linspace(min(X), max(X), 100)
y_line = 4 * x_line + 5
y_hat = w[0] + w[1] * x_line

plt.scatter(X, y)
plt.plot(x_line, y_line, color='green', label='y')
plt.plot(x_line, y_hat, color='red', label='y\'')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Generated Dataset')
plt.grid(True)
plt.legend()

plt.show()

"""### show gradient path"""

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
W0, W1 = np.meshgrid(w0_vals, w1_vals)
ax.plot_surface(W0, W1, np.log(J), cmap='viridis', alpha=0.5)
# ax.contourf(W0, W1, np.log(J),zdir='z',offset=np.log(J).min(),cmap='viridis')
ax.set_xlabel('w0')
ax.set_ylabel('w1')
ax.set_zlabel('log(J(w))')
plt.title("Cost Function Surface (Log Scale)")

# Plot the points on the 3D surface for each GD iteration
w_history_array = np.array(w_history)  # Convert list to array for easier slicing
w0_history = w_history_array[:, 0]
w1_history = w_history_array[:, 1]
cost_history_log = np.log(np.array(cost_history))  # Log of the cost history

# Plot the path of gradient descent in 3D
ax.plot(w0_history[:num_iters], w1_history[:num_iters], cost_history_log, marker='o', color='r', label='GD Path', markersize=3)

plt.legend()
plt.show()

plt.show()